---
title: "DMS_Meta"
author: "T.L"
date: "2024-05-07"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Libraries
```{r}

library(tidyverse) # data wrangling
library(janitor) # clean column names
library(metafor) # compute effect sizes
library(effectsize)# compute effect sizes
library(clubSandwich)
library(TOSTER)

```


# Clean up data
```{r}

data <- read.csv(file = "2024-0520_review_Formatted.csv", header = TRUE)

data <- clean_names(data)

sapply(data, class)

# Clean up culture column into separate 

df1 <- data %>% separate(culture_of_sample_study_1, 
                         c("culture_powerdistance", "culture_individualism",
                           "culture_mtas", "culture_uncertaintyavoidance",
                           "culture_longtermorientation", "culture_indulgence"), 
                         ";")

df1$culture_powerdistance<-gsub("PD:","",as.character(df1$culture_powerdistance))
df1$culture_individualism<-gsub("Individualism:","",as.character(df1$culture_individualism))
df1$culture_mtas<-gsub("MTAS:","",as.character(df1$culture_mtas))
df1$culture_uncertaintyavoidance<-gsub("UA:","",as.character(df1$culture_uncertaintyavoidance))
df1$culture_longtermorientation<-gsub("LTO:","",as.character(df1$culture_longtermorientation))
df1$culture_indulgence<-gsub("Indulgence:","",as.character(df1$culture_indulgence))

sapply(df1, class)

df1$culture_powerdistance <- as.numeric(df1$culture_powerdistance)
df1$culture_individualism <- as.numeric(df1$culture_individualism)
df1$culture_mtas <- as.numeric(df1$culture_mtas)
df1$culture_uncertaintyavoidance <- as.numeric(df1$culture_uncertaintyavoidance)
df1$culture_longtermorientation <- as.numeric(df1$culture_longtermorientation)
df1$culture_indulgence <- as.numeric(df1$culture_indulgence)

# Fix up some names

df1 <- df1 %>% rename(country = country_in_which_the_study_conducted)

df1$country<-gsub("Other:","",as.character(df1$country))

df1 <- df1 %>% rename(topic = topic_of_decision_important_general_unknown_etc_study_1)

df1 <- df1 %>% rename(measure = type_of_avoidant_decision_making_measure_study_1)

df1 <- df1 %>% rename(sample_type = population_description_study_1)

df1 <- df1 %>% rename(analysis = analysis_type_study_1_avoidant_dv)

df1 <- df1 %>% mutate_all(na_if, "")

write.csv(df1, file = "prepped_meta_data.csv")

```


# Compute Effect Sizes
## Function
```{r}

#' `compute_es()` creates "effect_sizes_pref.csv", by computing standardized effect sizes using the outcomes 
#' information (e.g., means, SDs, correlations) in 'clean_study_data_*.csv'.
#' @param data Data file for processing
#' @returns A data frame with details of all the studies of the specified econ. preference (e.g., first author, 
#' year of publication, sample size) along with the standardized effect size and their respective sampling variance
#' that is saved as a csv file in a designated folder. 

compute_es <- function(data) {
  
# EFFECT SIZES ----------------------------------------------------------

 # if (preference == "avoidance") {
   # means and SDS (young vs. old)
  smd_dat <- df1 %>% 
    subset(analysis == "Ms and SDs")
  
    # add point-biserial correlation and its variance (Soper's 'exact' equation) 
    # to dataset for avoidance
    smd_dat <- escalc(measure = "RPB",
                    m1i = young_adults_dv_mean, 
                    m2i = young_adults_dv_sd,
                    sd1i = older_adults_dv_mean,
                    sd2i = older_adults_dv_sd,
                    n1i = number_of_older_adults_categorical_var_study_1,
                    n2i = number_of_young_adults_categorical_var_study_1,
                    dat = smd_dat,
                    vtype = "LS",
                    replace=FALSE)
    
     #smd_dat_av_ado <- df1 %>% 
    #filter(age_variable_study_1 %in% c("Categorical (young/older groups")|
           #  groups == "Young/older adolescents")
    
  #  smd_dat_av_ado <- escalc(measure = "RPB",   #                 m1i = adolescents_younger_avoidance_mean, 
     #              m2i = adolescents_younger_avoidance_sd,
     #              sd1i = adolescents_older_avoidance_mean,
     #              sd2i = adolescents_older_avoidance_sd,
     #              n1i = number_of_older_adults_categorical_var_study_1,
     #              n2i = number_of_young_adults_categorical_var_study_1,
     #              dat = smd_dat_av_ado,
     #             vtype = "LS",
     #             replace=FALSE)
    
    smd_dat <- smd_dat %>% 
      filter(analysis == "Ms and SDs") %>% 
    # calculate age difference in decades (mean or midpoint in age range)
      mutate(dec_diff = case_when(!is.na(age_m_young_adults) &!is.na(age_m_older_adults) ~ .1*(age_m_older_adults-age_m_young_adults),
                                TRUE ~ .1*(((min_age_older+max_age_older)/2) - ((min_age_young+max_age_young)/2))),
           cor_type = "RPB")
    

    
    
 
    
      #####################################################_
  
  # correlations 
     cor_dat <-  df1 %>% 
       subset(df1$analysis %in% c("Correlation", "Both"))
# use correlations from extreme group designs if available
  
  # add yi=ri and vi (sampling variances) to dataset for avoidance
  cor_dat <- escalc(measure = 'COR',
                    ri = dv_age_result,
                    ni = total_number_of_participants_study_1,
                    vtype = "LS",
                    data = cor_dat)
  

  

  cor_dat <-  cor_dat %>% 
    # calculate age difference in decades
     mutate(age_min = case_when(analysis == "Both" ~ as.numeric(min_age_young),
                               TRUE ~ min_age), # in case age min/max info is unavailable
            age_max = case_when(analysis == "Both" ~ as.numeric(max_age_older),
                               TRUE ~ as.numeric(max_age)),   # in case age min/max info is unavailable
           cor_type = "COR",
           study_design = case_when(analysis == "Both" ~"extreme_group",
                                    analysis == "Corrleation" ~ "continuous")) %>% 
    mutate(dec_diff =  .1*(max_age - min_age))
  
  
  
   # bind data frames
   es <- bind_rows(smd_dat, cor_dat) %>%
     select(study_id, outcome_num, yi, vi, cor_type, age_variable_study_1, dec_diff) %>%
     rename(cor_yi = yi,
           cor_vi = vi) %>% 
     full_join(df1, by = c("study_id", "outcome_num")) %>% 
     mutate(author_extract = "Leon",
      es_id = 1:n(),  
           # number of participants included in the ES calculation
          n_incl_es = case_when(analysis %in% c("Both", "Correlation") ~ total_number_of_participants_study_1,
                                 # extreme group comparison included old vs. young differences
                                 analysis!= "Correlation" ~ number_of_older_adults_categorical_var_study_1 +
                                  number_of_young_adults_categorical_var_study_1))
   
   # Reverse effect sizes
      es <- es %>%  
        group_by(study_id) %>% 
      # assign a number to each study
       mutate(study_id_1 = cur_group_id()) %>% 
        ungroup() %>% 
        rowwise() %>% 
        mutate(reversed_es = case_when(!is.na(young_adults_dv_mean) 
                                          & !is.na(older_adults_dv_mean) &
                                            young_adults_dv_mean > older_adults_dv_mean ~ 1,
                                          !is.na(dv_age_result) & dv_age_result < 0 ~ 1,
                                          TRUE ~ 0))
      
      es <- es %>%
        mutate(cor_yi = ifelse(reversed_es == 1, cor_yi * -1, cor_yi))
   
#  }
  


      
#  }
  
  

  
    # SAVE OUTPUT ------------------------------------------------------------
  
  
   es_dat <- es %>% 
         group_by(study_id) %>% 
        mutate(study_es_id = 1:n()) %>% # create id for effect sizes within each study
        ungroup() %>% 
      # assign a number to each study label
      select(study_id, study_es_id, study_id_1, title, year_of_publication, country, measure, 
             dv, analysis,
             culture_powerdistance, culture_individualism, culture_mtas,
             culture_uncertaintyavoidance, culture_longtermorientation, culture_longtermorientation,
             culture_indulgence, topic, sample_type, groups, analysis, total_number_of_participants_study_1,
             number_of_older_adults_categorical_var_study_1, number_of_young_adults_categorical_var_study_1,
             gender_female_value,
             age_m_young_adults, age_m_older_adults, age_m_value, 
             min_age_young, min_age_older, min_age, 
             max_age_young, max_age_older, max_age, 
             dec_diff, young_adults_dv_mean, young_adults_dv_sd,
             older_adults_dv_mean, older_adults_dv_sd,
             dv_age_result, cor_yi, cor_vi, cor_type,
             es_id, n_incl_es, reversed_es, author_extract)
   
   
     
     es_dat$study_id_2 = es_dat$study_id
     
     es_dat <- es_dat %>% 
       separate(study_id_2, c("first_author", "year", "section"), " ")
  
   write_csv(es_dat, file = "effect_sizes.csv")
  
  print("effect_sizes_%s.csv created successfully! Saved in:   Home/PhD/Meta_analysis/Meta_Analyses") 
}
             
             
             
```

## Compute effect sizes for data
```{r}


df2 <- read.csv(file = "prepped_meta_data.csv")

compute_es(df2)

```


# Overview and visualisation
## Table function
```{r}

table_es_overview <- function(dat) {

  dat <- dat %>%
    mutate(pub_study = paste0(title, tolower(section)))
  

  dat_n <- dat %>% 
    distinct(study_id, n_incl_es, n_incl_es, pub_study) %>% 
    group_by(study_id) %>%  
    # within studies even if a sample completed the same task, some data points 
    # might go missing, so we select the highest number recorded within a same sample
    filter(n_incl_es == max(n_incl_es)) %>% 
    ungroup() 
  
  
  
  t <-  tibble(
    n_publications =  length(unique(paste0(dat$title, dat$first_author))),
    n_studies = length(unique(dat$pub_study)),
    n_es = nrow(dat),
    n_participants = sum(dat_n$n_incl_es),
    pub_range = paste0(as.character(min(dat$year))," - ", as.character(max(dat$year))))

  
  return(t)
  
  
  
}


```

## Visualisation and overview table of data
```{r}

dat <- read.csv(file = "effect_sizes.csv", header = TRUE)

## There are duplicate rows - Unsure why. ## FIXED IT!!



dat %>% 
  ggplot(aes(x= cor_yi)) + 
  geom_density() +
  theme_bw() +
  ggtitle("Estimate")

dat %>% 
  ggplot(aes(x= cor_vi)) + 
  geom_density() +
  theme_bw() +
  ggtitle("Estimate")

dat %>% 
  ggplot(aes(x= log10(n_incl_es))) + 
  geom_density() +
  theme_bw() +
  ggtitle("Sample")

dat %>% 
  ggplot(aes(x= dec_diff)) + 
  geom_density() +
  theme_bw() +
  ggtitle("Sample age Differences (decades)")



table1 <- table_es_overview(dat)

table1

```


# Analyses
## MLMA function
```{r}

#' `conduct_mlma()` computes a pooled effect size using the information from "effect_sizes.csv" by 
#' fitting a three-level meta-analytic model. 
#' Follows recommendations from: https://wviechtb.github.io/metafor/reference/misc-recs.html?q=sandw#details
#' @param dat A data frame containing effect sizes (i.e., "effect_sizes_pref.csv")
#' @param rho value of the correlation of the sampling errors within clusters
#' @returns a metafor rma.mv object.
#' @examples




# library(tidyverse) # for data wrangling
# library(metafor) # compute effect sizes


conduct_mlma <- function(dat, rho) {
  
  # transform data into an escalc object
  dat <- escalc(yi = cor_yi, vi = cor_vi, data = dat )
  dat$study_id_1 <- as.character(dat$study_id_1)
  dat$es_id <- as.character(dat$es_id)
  
  ### create approx. V matrix assuming that the effect sizes within studies
  ###  are correlated with a correlation coefficient of value 'rho'
  V_mat <- vcalc(vi = vi,
                 cluster = study_id_1,
                 obs = es_id,
                 data = dat,
                 rho = rho)
  
  ### fit multilevel model using the approximate V matrix
  res <- rma.mv(yi = yi,
                V = V_mat,
                random = ~ 1 | study_id_1/es_id, # random effect of study and estimate
                data=dat,
                # test = "t",
                # dfs="contain",
                control=list(iter.max=100)) 
  
  return(res)
  
}


```

## Sensitivity function
```{r}
#' `conduct_sensitivity_rho()` computes a pooled effect size using the information from "effect_sizes.csv" by 
#' fitting a multilevel meta-analytic model (applying robust variance estimation) and using different values of rho.
#' @param dat A data frame containing effect sizes (i.e., "effect_sizes.csv")
#' @param rho_vec numeric vector of values of the correlation of the sampling errors within clusters
#' @returns  a data frame with summary information of the pooled estimates
#' @examples




# library(tidyverse) # for data wrangling
# library(metafor) # compute effect sizes


conduct_sensitivity_rho <- function(dat, rho_vec) {
  
  # get the mlma function
  source("code/functions/conduct_mlma.R")
  
  # object to store output
  m <- NULL
  
  
  for (rho in rho_vec) {
    
    # fit multilevel model
    res <- conduct_mlma(dat = dat, rho = rho)
    
    
    # apply RVE methods
      res <- robust(res, cluster = study_id_1, clubSandwich = TRUE)
      
    
    #create output
    m <- bind_rows(m,
                   tibble(   # pref = unique(dat$pref),
                             rho_val = rho,
                             estimate = res$beta[1],
                             se = res$se,
                             zval = res$zval,
                             pval = res$pval,
                             ci.lb = res$ci.lb,
                             ci.ub = res$ci.ub,
                             QE = res$QE,
                             QEp = res$QEp,
                             tau2 = res$tau2,
                             I2 = res$I2,
                             H2 = res$H2))
    
    
  }
  
  
  
  return(m)
  
}

```


## CMA function
```{r}
#' `conduct_cma()` computes a pooled effect size using the information from "effect_sizes.csv" by 
#' fitting a multilevel meta-analytic model repetitively by adding the effect size(s) of a study based 
#' on the publication year. 
#' @param dat A data frame containing effect sizes (i.e., "effect_sizes.csv")
#' @param rho value of the correlation of the sampling errors within clusters
#' @param rve logical value to apply or not robust variance estimation
#' @returns a data frame with summary information of the pooled estimates
#' @examples

# 
# library(tidyverse) # for data wrangling
# library(metafor) # compute effect sizes
# 

conduct_cma <- function(dat, rho, rve) {
  
  # get the mlma function
  source("code/functions/conduct_mlma.R")
  
  # object to store output
  m <- NULL
  
  
  #order labels by year
  year_order <- dat %>% 
    distinct(study_id, year) %>% 
    arrange(year) %>% 
    mutate(study_id_1 = 1:n())
  
  dat <- dat %>% 
    select(-c(study_id_1, year)) %>% 
    left_join(year_order, by = "study_id") %>% 
    ungroup()
  
  
  nth_study <- 1
  
  for (i in 1:max(dat$study_id_1)) {
    
    # select subset of effect sizes
    sdat <- dat %>% filter(study_id_1 %in% c(1:i))
    
    # study being added
    added_study <- if_else(i == 1, unique(sdat$study_id[sdat$study_id_1 == i]), 
                           paste0("+ ",unique(sdat$study_id[sdat$study_id_1 == i])))
    
    
    # if first study, no need multilevel model: simply aggregate effect sizes
    #  and fit EE model to easily extract values of interest
    if (i == 1) {
      
      # transform data into an escalc object
      sdat <- escalc(yi = cor_yi, vi = cor_vi, data = sdat)
      
      # aggregate ES
      agg <- aggregate(sdat,
                       cluster=study_id_1,
                       rho=rho)
      
      res <- rma(yi = yi,
                 vi = vi,
                 data = agg, 
                 method="EE", 
                 slab= study_id)
      
    }
    
    
    # subsequent studies 
    if (i != 1) {
      
      # fit multilevel model
      res <- conduct_mlma(dat = sdat, rho = rho)
      
      # robust variance estimation
      if (isTRUE(rve)) {
        res <- robust(res, cluster = study_id_1, clubSandwich = TRUE)
      }
      
    }

    
    
    #create output
    m <- bind_rows(m,
                   tibble(#preference = unique(dat$pref),
                          slab = added_study,
                          k_added = nrow(dat[dat$study_id_1 == i,]), # number of ESs added
                          nth_study = nth_study, # how many of studies added so far
                          # fail_safe_n = fail_safe_n$fsnum, 
                          rve = rve,
                          estimate = res$beta[1],
                          se = res$se,
                          zval = res$zval,
                          pval = res$pval,
                          ci.lb = res$ci.lb,
                          ci.ub = res$ci.ub,
                          sigma_2_1 = ifelse(i != 1, res$sigma2[1], 0),
                          sigma_2_2 = ifelse(i != 1, res$sigma2[2], 0)))
    
    
    nth_study <- nth_study + 1
    
  } # study_id for loop
  
  return(m)
}

```


## To leave out a study
```{r}


#' `conduct_leave1out()` computes a pooled effect size by fitting a multilevel meta-analytic model repetitively by 
#' removing one study.
#' @param dat A data frame containing effect sizes (i.e., "effect_sizes.csv")
#' @param rho value of the correlation of the sampling errors within clusters
#' @param rve logical value to apply or not robust variance estimation
#' @returns a data frame with summary information of the pooled estimates
#' @examples



# library(tidyverse) # for data wrangling
# library(metafor) # compute effect sizes
# 

conduct_leave1out <- function(dat, rho, rve) {
  
  # get the mlma function
  source("code/functions/conduct_mlma.R")
  
  # object to store output
  m <- NULL
  
  for (i in 0:max(dat$study_id_1)) {
    
    # omit one study
    sdat <- dat %>% filter(study_id_1 != i)
    
    # study being omitted
    omit_study <- paste0("-",unique(dat$study_id[dat$study_id_1 == i]))
    year_of_publication  <- unique(dat$year[dat$study_id_1 == i])
    
    
    if (i == 0) {
      omit_study <- "Overall"
      year_of_publication <- NULL
    }

    
    # fit multilevel model
    res <- conduct_mlma(dat = sdat, rho = rho)
    
    # robust variance estimation
    if (isTRUE(rve)) {
      res <- robust(res, cluster = study_id_1, clubSandwich = TRUE)
    }
    
    
    
    #create output
    m <- bind_rows(m,
                   tibble(   #preference = unique(dat$pref),
                             slab = omit_study,
                             year_of_publication = year_of_publication,
                             estimate = res$beta[1],
                             se = res$se,
                             zval = res$tval,
                             df = res$ddf,
                             pval = res$pval,
                             ci.lb = res$ci.lb,
                             ci.ub = res$ci.ub,
                             Q = res$QE,
                             Qp = res$QEp))
    
    
  } # study_id for loop
  
  
  return(m)
}

```


### Multilevel meta-analysis
```{r}

#We use the effect sizes to fit a three-level meta-analysis model with random 
#effects at the study and estimate level in which we account for the dependence 
#of effect sizes by letting the sampling errors within studies to be correlated 
#with a correlation coefficient of 'rho'. We also apply robust variance 
#estimation (RVE) with small-sample adjustment.

rho = .5

  # read effect size datasets
  dat <- read.csv(file = "effect_sizes.csv", header = TRUE)
  
  # fitting multilevel model
  mlma <- conduct_mlma(dat = dat, rho = rho)
  mlma <- robust(mlma, cluster = study_id_1, clubSandwich = TRUE)
  
  # save output
  write_rds(mlma, "mlma_%s.rds")

  mlma
  
```


#### Equivalence testing
```{r}

#To assess whether the effect sizes we obtained are practically or theoretically 
#of *relevance* (i.e., does the overall effect size fall within the boundaries 
#of a smallest effect size of interest or not?) we conducted equivalence tests. 
#We determined the smallest effect size of interest to be |.1|

 m <- read_rds("mlma_%s.rds")
  
  rob_se <- m$se
  es <- m$b[1]
  
  t <- TOSTmeta(ES = es,
                se = rob_se,
                low_eqbound_d=-0.1,
                high_eqbound_d=0.1,
                alpha=0.05)
  
  # save output
  write_rds(t, "equivalence_test_%s.rds")

```


### Moderator testing
```{r}



# Culture - Pre-registered


# Gender - Pre-registered


# Decision topic - Pre-registered


# MDMQ or GDMS - Pre-registered


# Year of publication - Pre-registered


# Avoidance or dependence from GDMS

```


